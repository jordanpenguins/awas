{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87824870",
   "metadata": {},
   "source": [
    "<h1> Data Design and Streaming</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16e5e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import  StructType, StructField, StringType, LongType, DoubleType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import col, split, element_at, when,from_json, to_timestamp, coalesce,broadcast\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "hostip = \"192.168.0.116\" \n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('[Demo] Spark Streaming from Kafka into MongoDB')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "topic_1 = 'Camera_A'\n",
    "topic_2 = 'Camera_B'\n",
    "topic_3 = 'Camera_C'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554791ca",
   "metadata": {},
   "source": [
    "<h2>Database setup</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e0ffddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_client = MongoClient(hostip, 27017) \n",
    "\n",
    "\n",
    "db = db_client[\"fit3182_a2_db\"]\n",
    "collection = db[\"no_match_records\"]\n",
    "collection.drop()\n",
    "collection.create_index([(\"car_plate\", 1)])\n",
    "\n",
    "\n",
    "collection = db[\"violations\"]\n",
    "collection.drop()\n",
    "collection.create_index([(\"violation_id\",1)])\n",
    "\n",
    "\n",
    "\n",
    "def insert_data(file_name, db_name, collection_name, client):\n",
    "    file_df = pd.read_csv(file_name)\n",
    "\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]  # This will auto-create the collection if it doesn't exist\n",
    "    collection.drop()\n",
    "\n",
    "    for _, row in file_df.iterrows():\n",
    "        collection.insert_one(row.to_dict())\n",
    "\n",
    "insert_data(\"camera.csv\",\"fit3182_a2_db\",\"camera\",db_client)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5274267",
   "metadata": {},
   "source": [
    "## insert camera information to database\n",
    "insert camera information to mongdodb and create dataframe for the camera information to join with camera event stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "764e8d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   camera_id  latitude   longitude  position  speed_limit\n",
      "0        1.0  2.157731  102.660100     152.5        110.0\n",
      "1        2.0  2.162419  102.652455     153.5        110.0\n",
      "2        3.0  2.167353  102.644914     154.5         90.0\n",
      "root\n",
      " |-- camera_id: integer (nullable = true)\n",
      " |-- position: double (nullable = true)\n",
      " |-- speed_limit: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "db_client = MongoClient(hostip, 27017) \n",
    "db = db_client.fit3182_a2_db\n",
    "data = list(db.camera.find({}, { \"_id\": 0 }))\n",
    "\n",
    "pd_camera = pd.DataFrame(data)\n",
    "\n",
    "camera_df = spark.createDataFrame(pd_camera)\n",
    "\n",
    "camera_df = camera_df.select(\n",
    "    col(\"camera_id\").cast(IntegerType()).alias(\"camera_id\"),\n",
    "    col(\"position\"),\n",
    "    col(\"speed_limit\")\n",
    ").cache()\n",
    "\n",
    "print(pd_camera)\n",
    "camera_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c3748",
   "metadata": {},
   "source": [
    "## setting up consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3729ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"batch_id\", IntegerType(), True),\n",
    "    StructField(\"car_plate\", StringType(), True),\n",
    "    StructField(\"camera_id\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"speed_reading\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "def create_consumer(topic):\n",
    "    topic_stream = (\n",
    "        spark.readStream.format('kafka')\n",
    "        .option('kafka.bootstrap.servers', f'{hostip}:9092')\n",
    "        .option('subscribe', topic) \n",
    "        .load()\n",
    "    )\n",
    "    \n",
    "    modified_stream = (\n",
    "        topic_stream\n",
    "        .select(\n",
    "            from_json(col(\"value\").cast(\"string\"), json_schema).alias(\"data\")\n",
    "        )\n",
    "        .select(\"data.*\")\n",
    "        .withColumn(\"event_time\", to_timestamp(col(\"timestamp\")))\n",
    "        .withColumnRenamed(\"camera_id\", \"cam_id\")\n",
    "        .join(broadcast(camera_df), col(\"cam_id\") == col(\"camera_id\"))\n",
    "        .drop(\"cam_id\")\n",
    "    )\n",
    "    \n",
    "    return modified_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "065d4bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_stream_cam_a_df = create_consumer(topic_1)\n",
    "topic_stream_cam_b_df = create_consumer(topic_2)\n",
    "topic_stream_cam_c_df = create_consumer(topic_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285e2ff",
   "metadata": {},
   "source": [
    "## Join between stream a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aec2eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- left_event_id: string (nullable = true)\n",
      " |-- left_batch_id: integer (nullable = true)\n",
      " |-- left_car_plate: string (nullable = true)\n",
      " |-- left_timestamp: string (nullable = true)\n",
      " |-- left_speed_reading: double (nullable = true)\n",
      " |-- left_event_time: timestamp (nullable = true)\n",
      " |-- left_camera_id: integer (nullable = true)\n",
      " |-- left_position: double (nullable = true)\n",
      " |-- left_speed_limit: double (nullable = true)\n",
      " |-- right_event_id: string (nullable = true)\n",
      " |-- right_batch_id: integer (nullable = true)\n",
      " |-- right_car_plate: string (nullable = true)\n",
      " |-- right_timestamp: string (nullable = true)\n",
      " |-- right_speed_reading: double (nullable = true)\n",
      " |-- right_event_time: timestamp (nullable = true)\n",
      " |-- right_camera_id: integer (nullable = true)\n",
      " |-- right_position: double (nullable = true)\n",
      " |-- right_speed_limit: double (nullable = true)\n",
      " |-- distance_km: double (nullable = true)\n",
      " |-- time_diff_hrs: double (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- batch_id: integer (nullable = true)\n",
      " |-- car_plate: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- speed_reading: double (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- camera_id: integer (nullable = true)\n",
      " |-- position: double (nullable = true)\n",
      " |-- speed_limit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, unix_timestamp\n",
    "from pyspark.sql.functions import abs as spark_abs\n",
    "\n",
    "\n",
    "cam_a_watermarked = (\n",
    "    topic_stream_cam_a_df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "    .select([col(c).alias(f\"left_{c}\") for c in topic_stream_cam_a_df.columns])\n",
    ") \n",
    "\n",
    "cam_b_watermarked = (\n",
    "    topic_stream_cam_b_df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "    .select([col(c).alias(f\"right_{c}\") for c in topic_stream_cam_b_df.columns])\n",
    ") \n",
    "\n",
    "joined_stream_a_b = cam_a_watermarked.join(\n",
    "    cam_b_watermarked,\n",
    "    expr(\"\"\"\n",
    "        left_car_plate = right_car_plate AND\n",
    "        right_event_time > left_event_time AND\n",
    "        right_event_time <= left_event_time + interval 2 minutes\n",
    "    \"\"\"),\n",
    "    \"full_outer\"\n",
    ")\n",
    "\n",
    "\n",
    "stream_with_avg_speed_a_b = (\n",
    "    joined_stream_a_b\n",
    "    .filter(\"left_car_plate IS NOT NULL AND right_car_plate IS NOT NULL\")\n",
    "    .withColumn(\"distance_km\", spark_abs(col(\"left_position\") - col(\"right_position\")))\n",
    "    .withColumn(\"time_diff_hrs\", (unix_timestamp(\"right_event_time\") - unix_timestamp(\"left_event_time\")) / 3600)\n",
    "    .withColumn(\"avg_speed\", col(\"distance_km\") / col(\"time_diff_hrs\"))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "unmatched_records_a_b = joined_stream_a_b.filter(\n",
    "    \"left_car_plate IS NULL OR right_car_plate IS NULL\"\n",
    ").select(\n",
    "    coalesce(col(\"left_event_id\"), col(\"right_event_id\")).alias(\"event_id\"),\n",
    "    coalesce(col(\"left_batch_id\"), col(\"right_batch_id\")).alias(\"batch_id\"),\n",
    "    coalesce(col(\"left_car_plate\"), col(\"right_car_plate\")).alias(\"car_plate\"),\n",
    "    coalesce(col(\"left_timestamp\"), col(\"right_timestamp\")).alias(\"timestamp\"),\n",
    "    coalesce(col(\"left_speed_reading\"), col(\"right_speed_reading\")).alias(\"speed_reading\"),\n",
    "    coalesce(col(\"left_event_time\"), col(\"right_event_time\")).alias(\"event_time\"),\n",
    "    coalesce(col(\"left_camera_id\"), col(\"right_camera_id\")).alias(\"camera_id\"),\n",
    "    coalesce(col(\"left_position\"), col(\"right_position\")).alias(\"position\"),\n",
    "    coalesce(col(\"left_speed_limit\"), col(\"right_speed_limit\")).alias(\"speed_limit\"),\n",
    ")\n",
    "    \n",
    "stream_with_avg_speed_a_b.printSchema()\n",
    "unmatched_records_a_b.printSchema()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d53b83",
   "metadata": {},
   "source": [
    "## Join stream b and c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d354296",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_b_watermarked = (\n",
    "    topic_stream_cam_b_df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "    .select([col(c).alias(f\"left_{c}\") for c in topic_stream_cam_b_df.columns])\n",
    ") \n",
    "\n",
    "cam_c_watermarked = (\n",
    "    topic_stream_cam_c_df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "    .select([col(c).alias(f\"right_{c}\") for c in topic_stream_cam_c_df.columns])\n",
    ") \n",
    "\n",
    "joined_stream_b_c = cam_b_watermarked.join(\n",
    "    cam_c_watermarked,\n",
    "    expr(\"\"\"\n",
    "        left_car_plate = right_car_plate AND\n",
    "        right_event_time > left_event_time AND\n",
    "        right_event_time <= left_event_time + interval 2 minutes\n",
    "    \"\"\"),\n",
    "    \"full_outer\"\n",
    ")\n",
    "\n",
    "\n",
    "stream_with_avg_speed_b_c = (\n",
    "    joined_stream_b_c\n",
    "    .filter(\"left_car_plate IS NOT NULL AND right_car_plate IS NOT NULL\")\n",
    "    .withColumn(\"distance_km\", spark_abs(col(\"left_position\") - col(\"right_position\")))\n",
    "    .withColumn(\"time_diff_hrs\", (unix_timestamp(\"right_event_time\") - unix_timestamp(\"left_event_time\")) / 3600)\n",
    "    .withColumn(\"avg_speed\", col(\"distance_km\") / col(\"time_diff_hrs\"))\n",
    ")\n",
    "\n",
    "unmatched_records_b_c = joined_stream_b_c.filter(\n",
    "    \"left_car_plate IS NULL OR right_car_plate IS NULL\"\n",
    ").select(\n",
    "    coalesce(col(\"left_event_id\"), col(\"right_event_id\")).alias(\"event_id\"),\n",
    "    coalesce(col(\"left_batch_id\"), col(\"right_batch_id\")).alias(\"batch_id\"),\n",
    "    coalesce(col(\"left_car_plate\"), col(\"right_car_plate\")).alias(\"car_plate\"),\n",
    "    coalesce(col(\"left_timestamp\"), col(\"right_timestamp\")).alias(\"timestamp\"),\n",
    "    coalesce(col(\"left_speed_reading\"), col(\"right_speed_reading\")).alias(\"speed_reading\"),\n",
    "    coalesce(col(\"left_event_time\"), col(\"right_event_time\")).alias(\"event_time\"),\n",
    "    coalesce(col(\"left_camera_id\"), col(\"right_camera_id\")).alias(\"camera_id\"),\n",
    "    coalesce(col(\"left_position\"), col(\"right_position\")).alias(\"position\"),\n",
    "    coalesce(col(\"left_speed_limit\"), col(\"right_speed_limit\")).alias(\"speed_limit\"),\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1040979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pymongo import InsertOne,DeleteOne,ReplaceOne\n",
    "from pymongo.errors import PyMongoError\n",
    "import time\n",
    "\n",
    "def safe_bulk_write(collection, operations, max_retries=3, delay=1):\n",
    "    attempt = 0\n",
    "    if operations == [] :\n",
    "        return\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            collection.bulk_write(operations)\n",
    "            return\n",
    "        except PyMongoError as e:\n",
    "            attempt += 1\n",
    "            print(f\"[Retry {attempt}] Failed to write to '{collection}': {e}\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "\n",
    "\n",
    "class DbWriterJoin:\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.client = MongoClient(host= hostip, port=27017)  # replace with actual host/port\n",
    "        self.db = self.client[\"fit3182_a2_db\"]\n",
    "        self.buffer = []\n",
    "        self.drop_pair = []\n",
    "\n",
    "        return True\n",
    "\n",
    "    def process(self, row):\n",
    "        row_dict = row.asDict()\n",
    "     \n",
    "        if row_dict[\"avg_speed\"] > row_dict[\"right_speed_limit\"] :\n",
    "            violation_id = row_dict[\"left_event_id\"] + row_dict[\"right_event_id\"] \n",
    "    \n",
    "            record = {\n",
    "                \"violation_id\" : violation_id,\n",
    "                \"car_plate\": row_dict[\"left_car_plate\"],\n",
    "                \"camera_id_end\": row_dict[\"right_camera_id\"],\n",
    "                \"camera_id_start\": row_dict[\"left_camera_id\"],\n",
    "                \"timestamp_start\": row_dict[\"left_timestamp\"],\n",
    "                \"timestamp_end\": row_dict[\"right_timestamp\"],\n",
    "                \"speed_reading\": row_dict[\"avg_speed\"],\n",
    "                \"violation_type\" : \"average\"\n",
    "            }\n",
    "            self.buffer.append(ReplaceOne({\"violation_id\" : violation_id},record,upsert = True))\n",
    "\n",
    "        else :\n",
    "            self.drop_pair.append(row_dict)\n",
    "        \n",
    "        \n",
    "    def close(self, error):\n",
    "        if error is None  :\n",
    "            safe_bulk_write(self.db[\"violations\"], self.buffer, max_retries=3, delay=1)\n",
    "            if self.drop_pair :\n",
    "                print(f'drop pairs : ')\n",
    "                for i in self.drop_pair :\n",
    "                    print(i)\n",
    "        self.client.close()\n",
    "\n",
    "\n",
    "class DbWriterNoMatch:\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.client = MongoClient(host= hostip, port=27017)  # replace with actual host/port\n",
    "        self.db = self.client[\"fit3182_a2_db\"]\n",
    "        self.buffer = []\n",
    "        return True\n",
    "\n",
    "    def process(self, row):\n",
    "        row_dict = row.asDict()\n",
    "        \n",
    "        record = {\n",
    "            \"event_id\" : row_dict[\"event_id\"],\n",
    "            \"car_plate\": row_dict[\"car_plate\"],\n",
    "            \"camera_id\": row_dict[\"camera_id\"],\n",
    "            \"timestamp\": row_dict[\"timestamp\"],\n",
    "            \"speed_reading\": row_dict[\"speed_reading\"],\n",
    "            \"position\" : row_dict[\"position\"],\n",
    "            \"event_time\" : row_dict[\"event_time\"],\n",
    "            \"speed_limit\" : row_dict[\"speed_limit\"]\n",
    "        }\n",
    "\n",
    "   \n",
    "        self.buffer.append(ReplaceOne({\"event_id\" : row_dict[\"event_id\"]}, record ,upsert = True))\n",
    "     \n",
    "    def close(self, error):\n",
    "        if error is None :\n",
    "            \n",
    "            safe_bulk_write(self.db[\"no_match_records\"], self.buffer, max_retries=3, delay=1)\n",
    "    \n",
    "        self.client.close()\n",
    "        \n",
    "class DbWriterSingle:\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.client = MongoClient(host= hostip, port=27017)  # replace with actual host/port\n",
    "        self.db = self.client[\"fit3182_a2_db\"]\n",
    "        self.buffer_violation = []\n",
    "        self.buffer_nomatch = []\n",
    "        self.record_to_drop = []\n",
    "        return True\n",
    "\n",
    "    def process(self, row):\n",
    "        \n",
    "        row_dict = row.asDict()\n",
    "        \n",
    "        match_records = self.db.no_match_records.find({\"car_plate\": row_dict[\"car_plate\"]})\n",
    " \n",
    "        for record in match_records :\n",
    "            if abs(record[\"camera_id\"] - row_dict[\"camera_id\"]) == 1 :\n",
    "                t1 = record[\"event_time\"]\n",
    "                t2 = row_dict[\"event_time\"]\n",
    "                dif = t1-t2\n",
    "                seconds = abs(dif.total_seconds())\n",
    "         \n",
    "                if record[\"camera_id\"] > row_dict[\"camera_id\"] :\n",
    "                    speed_limit = record[\"speed_limit\"]\n",
    "                    camera_id_start = row_dict[\"camera_id\"]\n",
    "                    camera_id_end = record[\"camera_id\"]\n",
    "                    timestamp_start = row_dict[\"timestamp\"]\n",
    "                    timestamp_end = record[\"timestamp\"]\n",
    "                else :\n",
    "                    speed_limit = row_dict[\"speed_limit\"]\n",
    "                    camera_id_start = record[\"camera_id\"]\n",
    "                    camera_id_end = row_dict[\"camera_id\"]\n",
    "                    timestamp_start = record[\"timestamp\"]\n",
    "                    timestamp_end = row_dict[\"timestamp\"]\n",
    "\n",
    "                position1 = record[\"position\"]\n",
    "                position2 = row_dict[\"position\"]\n",
    "                distance = abs(position1-position2)\n",
    "                avg_speed = distance / (seconds/3600)\n",
    "\n",
    "                if avg_speed > speed_limit :\n",
    "                    violation_id = row_dict[\"event_id\"] + record[\"event_id\"] \n",
    "                                                       \n",
    "                    violation_record = {\n",
    "                        \"violation_id\" : violation_id,\n",
    "                        \"car_plate\": row_dict[\"car_plate\"],\n",
    "                        \"camera_id_start\": camera_id_start,\n",
    "                        \"camera_id_end\": camera_id_end,\n",
    "                        \"timestamp_start\": timestamp_start,\n",
    "                        \"timestamp_end\": timestamp_end,\n",
    "                        \"speed_reading\": avg_speed,\n",
    "                        \"violation_type\" : \"average\"\n",
    "                    }\n",
    "\n",
    "                                                       \n",
    "                    self.buffer_violation.append(ReplaceOne({\"violation_id\" : violation_id},violation_record ,upsert = True))\n",
    "                                                       \n",
    "                else :\n",
    "                    self.record_to_drop.append((record,row_dict))\n",
    "                                                    \n",
    "            \n",
    "                self.buffer_nomatch.append(DeleteOne({\"_id\": record[\"_id\"]}))\n",
    "                  \n",
    "                 \n",
    "        if row_dict[\"speed_limit\"] < row_dict[\"speed_reading\"] :\n",
    "            \n",
    "            \n",
    "            record = {\n",
    "                \"violation_id\" : row_dict[\"event_id\"],\n",
    "                \"car_plate\": row_dict[\"car_plate\"],\n",
    "                \"camera_id_end\": row_dict[\"camera_id\"],\n",
    "                \"camera_id_start\": row_dict[\"camera_id\"],\n",
    "                \"timestamp_start\": row_dict[\"timestamp\"],\n",
    "                \"timestamp_end\": row_dict[\"timestamp\"],\n",
    "                \"speed_reading\": row_dict[\"speed_reading\"],\n",
    "                \"violation_type\" : \"instantaneous\"\n",
    "            }\n",
    "                                                       \n",
    "            self.buffer_violation.append(ReplaceOne({\"violation_id\" : row_dict[\"event_id\"]},record,upsert = True))\n",
    "\n",
    "                                      \n",
    "    def close(self, error):\n",
    "        if error is None :                                               \n",
    "            safe_bulk_write(self.db[\"violations\"], self.buffer_violation, max_retries=3, delay=1)\n",
    "            safe_bulk_write(self.db[\"no_match_records\"], self.buffer_nomatch, max_retries=3, delay=1)\n",
    "            if self.record_to_drop :                            \n",
    "                print(f'dropped pairs : ')\n",
    "                for i in self.record_to_drop :\n",
    "                    print(i)\n",
    "                                                       \n",
    "        self.client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dbbd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS ONE\n",
    "\n",
    "stream_with_average = stream_with_avg_speed_a_b.union(stream_with_avg_speed_b_c)\n",
    "\n",
    "union_cam_a_b_c = topic_stream_cam_a_df.union(topic_stream_cam_b_df).union(topic_stream_cam_c_df)\n",
    "\n",
    "no_match_record_union = unmatched_records_b_c.union(unmatched_records_a_b)\n",
    "\n",
    "\n",
    "no_match_record = (\n",
    "    no_match_record_union\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(DbWriterNoMatch())\n",
    ")\n",
    "\n",
    "\n",
    "join_writer = ( \n",
    "    stream_with_average\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(DbWriterJoin())\n",
    ")\n",
    "\n",
    "single_record_writer = (\n",
    "    union_cam_a_b_c\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(DbWriterSingle())\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    query_1 = no_match_record.start()\n",
    "    query_2 = join_writer.start()\n",
    "    query_3 = single_record_writer.start()\n",
    "\n",
    "    \n",
    "    query_1.awaitTermination()\n",
    "    query_2.awaitTermination()\n",
    "    query_3.awaitTermination()\n",
    "\n",
    "\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopped query')\n",
    "except StreamingQueryException as exc:\n",
    "    print(exc)\n",
    "finally:\n",
    "    query_1.stop()\n",
    "    query_2.stop()\n",
    "    query_3.stop()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1621fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "stream_with_average = stream_with_avg_speed_a_b.union(stream_with_avg_speed_b_c)\n",
    "\n",
    "union_cam_a_b_c = topic_stream_cam_a_df.union(topic_stream_cam_b_df).union(topic_stream_cam_c_df)\n",
    "\n",
    "no_match_record_union = unmatched_records_b_c.union(unmatched_records_a_b).withColumn(\"log_info\", lit(\"no match\"))\n",
    "\n",
    "\n",
    "\n",
    "dropped_record = no_match_record_union.union(non_instant_violation)\n",
    "\n",
    "# no matching record\n",
    "drop_logger_join = (\n",
    "    no_match_record_union\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .format('console')\n",
    ")\n",
    "\n",
    "drop_logger_single = (\n",
    "    dropped_record\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .format('console')\n",
    ")\n",
    "\n",
    "\n",
    "# violation of average speed\n",
    "joined_pair_writer = ( \n",
    "    average_violation\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(DbWriterJoin())\n",
    ")\n",
    "\n",
    "#instant violation\n",
    "single_record_writer = (\n",
    "    instant_violation\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(DbWriterSingle())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_with_average = stream_with_avg_speed_a_b.union(stream_with_avg_speed_b_c)\n",
    "\n",
    "union_cam_a_b_c = topic_stream_cam_a_df.union(topic_stream_cam_b_df).union(topic_stream_cam_c_df)\n",
    "\n",
    "no_match_record_union = unmatched_records_b_c.union(unmatched_records_a_b))\n",
    "\n",
    "non_instant_violation = union_cam_a_b_c.filter(col(\"speed_limit\") > col(\"speed_reading\")).withColumn(\"log_info\", lit(\"no instant violation\"))\n",
    "instant_violation = union_cam_a_b_c.filter(col(\"speed_limit\") <= col(\"speed_reading\")).withColumn(\"log_info\", lit(\"no instant violation\"))\n",
    "\n",
    "non_average_violation = stream_with_average.filter(col(\"right_speed_limit\") > col(\"avg_speed\")).withColumn(\"log_info\", lit(\"no average violation\"))\n",
    "average_violation = stream_with_average.filter(col(\"right_speed_limit\") < col(\"avg_speed\"))\n",
    "\n",
    "\n",
    "dropped_record = no_match_record_union.union(non_instant_violation)\n",
    "\n",
    "no_match_record_writer = (\n",
    "    no_match_record\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(DbWriterDropRecord())\n",
    ")\n",
    "\n",
    "\n",
    "# no matching record\n",
    "drop_logger_join = (\n",
    "    non_average_violation\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .format('console')\n",
    ")\n",
    "\n",
    "drop_logger_single = (\n",
    "    dropped_record\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .format('console')\n",
    ")\n",
    "\n",
    "\n",
    "# violation of average speed\n",
    "joined_pair_writer = ( \n",
    "    average_violation\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(DbWriterJoin())\n",
    ")\n",
    "\n",
    "#instant violation\n",
    "single_record_writer = (\n",
    "    instant_violation\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(DbWriterSingle())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3cd303",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    query_1 = joined_pair_writer.start()\n",
    "    query_2 = single_record_writer.start()\n",
    "    query_3 = drop_logger_join.start()\n",
    "    query_4 = drop_logger_single.start()\n",
    "    \n",
    "    query_1.awaitTermination()\n",
    "    query_2.awaitTermination()\n",
    "    query_3.awaitTermination()\n",
    "    query_4.awaitTermination()\n",
    "\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopped query')\n",
    "except StreamingQueryException as exc:\n",
    "    print(exc)\n",
    "finally:\n",
    "    query_1.stop()\n",
    "    query_2.stop()\n",
    "    query_3.stop()\n",
    "    query_4.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b85d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
