{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87824870",
   "metadata": {},
   "source": [
    "<h1> Data Design and Streaming</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16e5e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import  StructType, StructField, StringType, LongType, DoubleType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import col, split, element_at, when,from_json, to_timestamp, coalesce,broadcast\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "hostip = \"192.168.0.116\" \n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('[Demo] Spark Streaming from Kafka into MongoDB')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "topic_1 = 'Camera_A'\n",
    "topic_2 = 'Camera_B'\n",
    "topic_3 = 'Camera_C'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554791ca",
   "metadata": {},
   "source": [
    "<h2>Database setup</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e0ffddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_client = MongoClient(hostip, 27017) \n",
    "\n",
    "def insert_data(file_name, db_name, collection_name, client):\n",
    "    file_df = pd.read_csv(file_name)\n",
    "\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]  # This will auto-create the collection if it doesn't exist\n",
    "    collection.drop()\n",
    "\n",
    "    for _, row in file_df.iterrows():\n",
    "        collection.insert_one(row.to_dict())\n",
    "\n",
    "insert_data(\"camera.csv\",\"fit3182_a2_db\",\"camera\",db_client)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5274267",
   "metadata": {},
   "source": [
    "## insert camera information to database\n",
    "insert camera information to mongdodb and create dataframe for the camera information to join with camera event stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "764e8d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   camera_id  latitude   longitude  position  speed_limit\n",
      "0        1.0  2.157731  102.660100     152.5        110.0\n",
      "1        2.0  2.162419  102.652455     153.5        110.0\n",
      "2        3.0  2.167353  102.644914     154.5         90.0\n",
      "root\n",
      " |-- camera_id: integer (nullable = true)\n",
      " |-- position: double (nullable = true)\n",
      " |-- speed_limit: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "db_client = MongoClient(hostip, 27017) \n",
    "db = db_client.fit3182_a2_db\n",
    "data = list(db.camera.find({}, { \"_id\": 0 }))\n",
    "\n",
    "pd_camera = pd.DataFrame(data)\n",
    "\n",
    "camera_df = spark.createDataFrame(pd_camera)\n",
    "\n",
    "camera_df = camera_df.select(\n",
    "    col(\"camera_id\").cast(IntegerType()).alias(\"camera_id\"),\n",
    "    col(\"position\"),\n",
    "    col(\"speed_limit\")\n",
    ").cache()\n",
    "\n",
    "print(pd_camera)\n",
    "camera_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c3748",
   "metadata": {},
   "source": [
    "## setting up consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3729ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"batch_id\", IntegerType(), True),\n",
    "    StructField(\"car_plate\", StringType(), True),\n",
    "    StructField(\"camera_id\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"speed_reading\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "def create_consumer(topic):\n",
    "    topic_stream = (\n",
    "        spark.readStream.format('kafka')\n",
    "        .option('kafka.bootstrap.servers', f'{hostip}:9092')\n",
    "        .option('subscribe', topic) # Subscribe to the topics here (work with camera A and B first)\n",
    "        .load()\n",
    "    )\n",
    "    \n",
    "    modified_stream = (\n",
    "        topic_stream\n",
    "        .select(\n",
    "            from_json(col(\"value\").cast(\"string\"), json_schema).alias(\"data\")\n",
    "        )\n",
    "        .select(\"data.*\")\n",
    "        .withColumn(\"event_time\", to_timestamp(col(\"timestamp\")))\n",
    "        .withColumnRenamed(\"camera_id\", \"cam_id\")\n",
    "        .join(broadcast(camera_df), col(\"cam_id\") == col(\"camera_id\"))\n",
    "        .drop(\"cam_id\")\n",
    "    )\n",
    "    \n",
    "    return modified_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "065d4bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_stream_cam_a_df = create_consumer(topic_1)\n",
    "topic_stream_cam_b_df = create_consumer(topic_2)\n",
    "topic_stream_cam_c_df = create_consumer(topic_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285e2ff",
   "metadata": {},
   "source": [
    "## Join between stream a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8aec2eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- left_event_id: string (nullable = true)\n",
      " |-- left_batch_id: integer (nullable = true)\n",
      " |-- left_car_plate: string (nullable = true)\n",
      " |-- left_timestamp: string (nullable = true)\n",
      " |-- left_speed_reading: double (nullable = true)\n",
      " |-- left_event_time: timestamp (nullable = true)\n",
      " |-- left_camera_id: integer (nullable = true)\n",
      " |-- left_position: double (nullable = true)\n",
      " |-- left_speed_limit: double (nullable = true)\n",
      " |-- right_event_id: string (nullable = true)\n",
      " |-- right_batch_id: integer (nullable = true)\n",
      " |-- right_car_plate: string (nullable = true)\n",
      " |-- right_timestamp: string (nullable = true)\n",
      " |-- right_speed_reading: double (nullable = true)\n",
      " |-- right_event_time: timestamp (nullable = true)\n",
      " |-- right_camera_id: integer (nullable = true)\n",
      " |-- right_position: double (nullable = true)\n",
      " |-- right_speed_limit: double (nullable = true)\n",
      " |-- distance_km: double (nullable = true)\n",
      " |-- time_diff_hrs: double (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- batch_id: integer (nullable = true)\n",
      " |-- car_plate: string (nullable = true)\n",
      " |-- camera_id: integer (nullable = true)\n",
      " |-- speed_reading: double (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- speed_limit: double (nullable = true)\n",
      " |-- position: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr,abs,unix_timestamp\n",
    "cam_a_watermarked = (\n",
    "    topic_stream_cam_a_df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "    .select([col(c).alias(f\"left_{c}\") for c in topic_stream_cam_a_df.columns])\n",
    ") \n",
    "\n",
    "cam_b_watermarked = (\n",
    "    topic_stream_cam_b_df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "    .select([col(c).alias(f\"right_{c}\") for c in topic_stream_cam_b_df.columns])\n",
    ") \n",
    "\n",
    "joined_stream_a_b = cam_a_watermarked.join(\n",
    "    cam_b_watermarked,\n",
    "    expr(\"\"\"\n",
    "        left_car_plate = right_car_plate AND\n",
    "        right_event_time > left_event_time AND\n",
    "        right_event_time <= left_event_time + interval 2 minutes\n",
    "    \"\"\"),\n",
    "    \"full_outer\"\n",
    ")\n",
    "\n",
    "\n",
    "stream_with_avg_speed_a_b = (\n",
    "    joined_stream_a_b\n",
    "    .filter(\"left_car_plate IS NOT NULL AND right_car_plate IS NOT NULL\")\n",
    "    .withColumn(\"distance_km\", abs(col(\"left_position\") - col(\"right_position\")))\n",
    "    .withColumn(\"time_diff_hrs\", (unix_timestamp(\"right_event_time\") - unix_timestamp(\"left_event_time\")) / 3600)\n",
    "    .withColumn(\"avg_speed\", col(\"distance_km\") / col(\"time_diff_hrs\"))\n",
    ")\n",
    "\n",
    "unmatched_records_a_b = joined_stream_a_b.filter(\n",
    "    \"left_car_plate IS NULL OR right_car_plate IS NULL\"\n",
    ").select(\n",
    "    coalesce(col(\"left_event_id\"), col(\"right_event_id\")).alias(\"event_id\"),\n",
    "    coalesce(col(\"left_batch_id\"), col(\"right_batch_id\")).alias(\"batch_id\"),\n",
    "    coalesce(col(\"left_car_plate\"), col(\"right_car_plate\")).alias(\"car_plate\"),\n",
    "    coalesce(col(\"left_camera_id\"), col(\"right_camera_id\")).alias(\"camera_id\"),\n",
    "    coalesce(col(\"left_speed_reading\"), col(\"right_speed_reading\")).alias(\"speed_reading\"),\n",
    "    coalesce(col(\"left_timestamp\"), col(\"right_timestamp\")).alias(\"timestamp\"),\n",
    "    coalesce(col(\"left_speed_limit\"), col(\"right_speed_limit\")).alias(\"speed_limit\"),\n",
    "    coalesce(col(\"left_position\"), col(\"right_position\")).alias(\"position\")\n",
    ")\n",
    "    \n",
    "stream_with_avg_speed_a_b.printSchema()\n",
    "unmatched_records_a_b.printSchema()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d53b83",
   "metadata": {},
   "source": [
    "## Join stream b and c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d354296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- left_event_id: string (nullable = true)\n",
      " |-- left_batch_id: integer (nullable = true)\n",
      " |-- left_car_plate: string (nullable = true)\n",
      " |-- left_timestamp: string (nullable = true)\n",
      " |-- left_speed_reading: double (nullable = true)\n",
      " |-- left_event_time: timestamp (nullable = true)\n",
      " |-- left_camera_id: integer (nullable = true)\n",
      " |-- left_position: double (nullable = true)\n",
      " |-- left_speed_limit: double (nullable = true)\n",
      " |-- right_event_id: string (nullable = true)\n",
      " |-- right_batch_id: integer (nullable = true)\n",
      " |-- right_car_plate: string (nullable = true)\n",
      " |-- right_timestamp: string (nullable = true)\n",
      " |-- right_speed_reading: double (nullable = true)\n",
      " |-- right_event_time: timestamp (nullable = true)\n",
      " |-- right_camera_id: integer (nullable = true)\n",
      " |-- right_position: double (nullable = true)\n",
      " |-- right_speed_limit: double (nullable = true)\n",
      " |-- distance_km: double (nullable = true)\n",
      " |-- time_diff_hrs: double (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- batch_id: integer (nullable = true)\n",
      " |-- car_plate: string (nullable = true)\n",
      " |-- camera_id: integer (nullable = true)\n",
      " |-- speed_reading: double (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- speed_limit: double (nullable = true)\n",
      " |-- position: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cam_b_watermarked = (\n",
    "    topic_stream_cam_b_df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "    .select([col(c).alias(f\"left_{c}\") for c in topic_stream_cam_b_df.columns])\n",
    ") \n",
    "\n",
    "cam_c_watermarked = (\n",
    "    topic_stream_cam_c_df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "    .select([col(c).alias(f\"right_{c}\") for c in topic_stream_cam_c_df.columns])\n",
    ") \n",
    "\n",
    "joined_stream_b_c = cam_b_watermarked.join(\n",
    "    cam_c_watermarked,\n",
    "    expr(\"\"\"\n",
    "        left_car_plate = right_car_plate AND\n",
    "        right_event_time > left_event_time AND\n",
    "        right_event_time <= left_event_time + interval 2 minutes\n",
    "    \"\"\"),\n",
    "    \"full_outer\"\n",
    ")\n",
    "\n",
    "\n",
    "stream_with_avg_speed_b_c = (\n",
    "    joined_stream_b_c\n",
    "    .filter(\"left_car_plate IS NOT NULL AND right_car_plate IS NOT NULL\")\n",
    "    .withColumn(\"distance_km\", abs(col(\"left_position\") - col(\"right_position\")))\n",
    "    .withColumn(\"time_diff_hrs\", (unix_timestamp(\"right_event_time\") - unix_timestamp(\"left_event_time\")) / 3600)\n",
    "    .withColumn(\"avg_speed\", col(\"distance_km\") / col(\"time_diff_hrs\"))\n",
    ")\n",
    "\n",
    "unmatched_records_b_c = joined_stream_b_c.filter(\n",
    "    \"left_car_plate IS NULL OR right_car_plate IS NULL\"\n",
    ").select(\n",
    "    coalesce(col(\"left_event_id\"), col(\"right_event_id\")).alias(\"event_id\"),\n",
    "    coalesce(col(\"left_batch_id\"), col(\"right_batch_id\")).alias(\"batch_id\"),\n",
    "    coalesce(col(\"left_car_plate\"), col(\"right_car_plate\")).alias(\"car_plate\"),\n",
    "    coalesce(col(\"left_camera_id\"), col(\"right_camera_id\")).alias(\"camera_id\"),\n",
    "    coalesce(col(\"left_speed_reading\"), col(\"right_speed_reading\")).alias(\"speed_reading\"),\n",
    "    coalesce(col(\"left_timestamp\"), col(\"right_timestamp\")).alias(\"timestamp\"),\n",
    "    coalesce(col(\"left_speed_limit\"), col(\"right_speed_limit\")).alias(\"speed_limit\"),\n",
    "    coalesce(col(\"left_position\"), col(\"right_position\")).alias(\"position\")\n",
    ")\n",
    "    \n",
    "stream_with_avg_speed_b_c.printSchema()\n",
    "unmatched_records_b_c.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1040979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import InsertOne\n",
    "class DbWriterJoin:\n",
    "    def open(self):\n",
    "        self.client = MongoClient(host= hostip, port=27017)  # replace with actual host/port\n",
    "        self.db = self.client['fit3182_db']\n",
    "        self.buffer = []\n",
    "        self.dropped_record = []\n",
    "        return True\n",
    "\n",
    "    def process(self, row):\n",
    "        row_dict = row.asDict()\n",
    "        print(f'row dict : {row_dict}')\n",
    "     \n",
    "        if row_dict[\"avg_speed\"] > row_dict[\"right_speed_limit\"] :\n",
    "            \n",
    "            # unique_id = f\"{row_dict[\"left_event_id\"]}_{row_dict[\"right_event_id\"]}\"\n",
    "\n",
    "            record = {\n",
    "                \"car_plate\": row_dict[\"left_car_plate\"],\n",
    "                \"camera_id_end\": row_dict[\"right_camera_id\"],\n",
    "                \"camera_id_start\": row_dict[\"left_camera_id\"],\n",
    "                \"timestamp_start\": row_dict[\"left_timestamp\"],\n",
    "                \"timestamp_end\": row_dict[\"right_timestamp\"],\n",
    "                \"speed_reading\": row_dict[\"avg_speed\"],\n",
    "            }\n",
    "\n",
    "            self.buffer.append(InsertOne(record))\n",
    "        else :\n",
    "            self.dropped_record.append(row_dict)\n",
    "      \n",
    "        \n",
    "    def close(self, error):\n",
    "        if error is None and self.buffer:\n",
    "            self.db[\"violations\"].bulk_write(self.buffer)\n",
    "            print(f'bufferx : {self.buffer}')\n",
    "            print(f'pairs that does not violate average speed :{self.dropped_record}')\n",
    "        self.client.close()\n",
    "\n",
    "class DbWriterSingle:\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.client = MongoClient(host = hostip, port=27017)  # replace with actual host/port\n",
    "        self.db = self.client['fit3182_db']\n",
    "        self.buffer = []\n",
    "        self.dropped_record = []\n",
    "        return True\n",
    "\n",
    "    def process(self, row):\n",
    "        row_dict = row.asDict()\n",
    "        if row_dict[\"speed_reading\"] > row_dict[\"speed_limit\"] :\n",
    "            record = {\n",
    "                \"car_plate\": row_dict[\"car_plate\"],\n",
    "                \"camera_id_end\": row_dict[\"camera_id\"],\n",
    "                \"camera_id_start\": row_dict[\"camera_id\"],\n",
    "                \"timestamp_start\": row_dict[\"timestamp\"],\n",
    "                \"timestamp_end\": row_dict[\"timestamp\"],\n",
    "                \"speed_reading\": row_dict[\"speed_reading\"],\n",
    "            }\n",
    "\n",
    "            self.buffer.append(\n",
    "                InsertOne(record)\n",
    "            )\n",
    "            \n",
    "        else :\n",
    "            self.dropped_record.append(row_dict)\n",
    "\n",
    "\n",
    "    def close(self, error):\n",
    "        if error is None and self.buffer:\n",
    "            self.db[\"violations\"].bulk_write(self.buffer)\n",
    "            print(f'dropped record (no violation) : {self.dropped_record}')\n",
    "                  \n",
    "        self.client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61dbbd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_with_average = stream_with_avg_speed_a_b.union(stream_with_avg_speed_b_c) \n",
    "\n",
    "union_cam_a_b_c = topic_stream_cam_a_df.union(topic_stream_cam_b_df).union(topic_stream_cam_a_df) \n",
    "\n",
    "no_match_record = unmatched_records_b_c.union(unmatched_records_a_b)\n",
    "\n",
    "\n",
    "# no matching record\n",
    "drop_logger = (\n",
    "    no_match_record\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .format('console')\n",
    ")\n",
    "\n",
    "\n",
    "# violation of average speed\n",
    "joined_pair_writer = ( \n",
    "    stream_with_average\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(DbWriterJoin())\n",
    ")\n",
    "\n",
    "#instant violation\n",
    "single_record_writer = (\n",
    "    union_cam_a_b_c\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(DbWriterSingle())\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c3cd303",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StreamingQueryException' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m query_3 \u001b[38;5;241m=\u001b[39m drop_logger\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mquery_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m query_2\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Query [id = f2d1b936-c384-4e84-8460-be40f949c7c8, runId = 8be4e2ae-146c-4975-85b9-23a8c7949a45] terminated with exception: Writing job aborted",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInterrupted by CTRL-C. Stopped query\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mStreamingQueryException\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(exc)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StreamingQueryException' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query_1 = joined_pair_writer.start()\n",
    "    query_2 = single_record_writer.start()\n",
    "    query_3 = drop_logger.start()\n",
    "    \n",
    "    query_1.awaitTermination()\n",
    "    query_2.awaitTermination()\n",
    "    query_3.awaitTermination()\n",
    "\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopped query')\n",
    "except StreamingQueryException as exc:\n",
    "    print(exc)\n",
    "finally:\n",
    "    query_1.stop()\n",
    "    query_2.stop()\n",
    "    query_3.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b468525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117ee34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
