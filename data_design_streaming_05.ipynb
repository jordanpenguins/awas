{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87824870",
   "metadata": {},
   "source": [
    "# Data Design and Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16e5e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import  StructType, StructField, StringType, LongType, DoubleType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import col, split, element_at, when,from_json, to_timestamp, coalesce,broadcast, expr, unix_timestamp\n",
    "from pyspark.sql.functions import abs as spark_abs\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "hostip = \"172.28.144.1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554791ca",
   "metadata": {},
   "source": [
    "## Database setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12ac2c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client = MongoClient(hostip, 27017) \n",
    "db = db_client[\"fit3182_a2_db\"]\n",
    "\n",
    "def insert_data(file_name: str, db_name: str, collection_name: str, client: MongoClient) -> None:\n",
    "    \"\"\"\n",
    "    Reads data from a CSV file and inserts it into a specified MongoDB collection.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    file_name (str): The path to the CSV file containing the data to be inserted.\n",
    "        \n",
    "    db_name (str) : The name of the MongoDB database where the collection resides.\n",
    "        \n",
    "    collection_name (str) : The name of the MongoDB collection to insert data into. \n",
    "        \n",
    "    client (pymongo.MongoClient) : MongoDB client instance used to connect to the database.    \n",
    "    \"\"\"\n",
    "    \n",
    "    # use panda to help parse csv\n",
    "    file_df = pd.read_csv(file_name)\n",
    "\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]  \n",
    "    collection.drop()\n",
    "    \n",
    "    # Convert each row of the DataFrame into a dictionary\n",
    "    documents = [row.to_dict() for _, row in file_df.iterrows()]\n",
    "    \n",
    "    collection.insert_many(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48cfead",
   "metadata": {},
   "source": [
    "Insert static data provided from the csv : camera and vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_data(\"camera.csv\",\"fit3182_a2_db\",\"camera\",db_client)\n",
    "\n",
    "insert_data(\"vehicle.csv\",\"fit3182_a2_db\",\"vehicle\",db_client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb2311",
   "metadata": {},
   "source": [
    "Initializes two MongoDB collections: **no_match_records** and **violations**.  \n",
    "\n",
    "Each collection is indexed on a key field that is commonly used in insertions, updates, or queries later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db[\"no_match_records\"]\n",
    "collection.drop()\n",
    "collection.create_index([(\"car_plate\", 1)])\n",
    "\n",
    "\n",
    "collection = db[\"violations\"]\n",
    "collection.drop()\n",
    "collection.create_index([(\"violation_id\",1)])\n",
    "collection.create_index([(\"car_plate\",1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03509c",
   "metadata": {},
   "source": [
    "# Event Stream Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a799f74d",
   "metadata": {},
   "source": [
    "## Instantaneous Speed Violation Detection\n",
    "\n",
    "Three Kafka consumers are set up to listen to events from a Kafka producer that broadcasts camera events from Points A, B, and C \n",
    "this would be **topic_stream_cam_a_df** , **topic_stream_cam_b_df**, **topic_stream_cam_c_df**. Each stream will joined with a static camera metadata DataFrame **camera_df**, which provides the speed limit for the corresponding camera.\n",
    "\n",
    "Once joined, each of the three data streams is sent directly to a MongoDB sink (**DBWriterSingle()**). Since each event now contains both the vehicle’s recorded speed and the speed limit (from the camera metadata), the violation check is performed by simply comparing the two. If the vehicle exceeds the speed limit, the event is stored in the MongoDB database as a violation, if it doesnt, it will simply be logged. No stream-to-stream joining is needed for this step.\n",
    "\n",
    "## Average Speed Violation Detection\n",
    "\n",
    "To detect average speed violations, where a vehicle passes through multiple camera points, stream-to-stream joins are necessary to correlate events from different camera locations.\n",
    "\n",
    "This is implemented using PySpark Structured Streaming's stream-stream join functionality:\n",
    "\n",
    "* **topic_stream_cam_a_df** is joined with **topic_stream_cam_b_df**\n",
    "\n",
    "* **topic_stream_cam_b_df** is joined with **topic_stream_cam_c_df**\n",
    "\n",
    "These joins would be on the car_plate and are performed within a defined time window.\n",
    "\n",
    "\n",
    "### State Management with Watermarks\n",
    "\n",
    "Because stream-stream joins require holding data to wait for matching records, watermarking is used to manage state to make sure data doesnt sit too long on the state.\n",
    "\n",
    "A 10-minute watermark is set, which means, For any given stream, if an incoming event's timestamp is more than 10 minutes earlier than the maximum event time seen so far, it will be considered too late to match. Since a full outer join is used, unmatched records are emitted with null on one side of the join.\n",
    "\n",
    "Unmatched records emitted from the stream-stream join are filtered into a separate stream and sent to a dedicated MongoDB sink (**DBWriterNoMatch()**), which writes them to the **no_match_record** collection. When future events arrive, they query this collection to check for a corresponding unmatched record. If a match is found, the system attempts to reconcile the two records.\n",
    "\n",
    "If an event arrives after the watermark threshold, it will not participate in any join logic at all, it is ignored for average speed calculation. However, it can still be processed for instantaneous speed checking, since that logic is independent of joins.\n",
    "\n",
    "Matched records resulting from the stream-stream join are routed into a dedicated stream for average speed calculation. This stream computes the average speed between two camera points and sends the results to a MongoDB sink (**DBWriterJoin()**). If the calculated speed exceeds the speed limit, the record is stored in the violation collection. Otherwise, the event is simply logged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0ffddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'violation_id_1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('Spark Streaming Violation Detection')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Define Kafka topics corresponding to the three camera event sources\n",
    "topic_1 = 'Camera_A'\n",
    "topic_2 = 'Camera_B'\n",
    "topic_3 = 'Camera_C'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3140a407",
   "metadata": {},
   "source": [
    "Create a Spark DataFrame containing camera information, which will later be used to join with the camera event stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "764e8d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- camera_id: integer (nullable = true)\n",
      " |-- position: double (nullable = true)\n",
      " |-- speed_limit: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd_camera = pd.read_csv(\"camera.csv\")\n",
    "\n",
    "camera_df = spark.createDataFrame(pd_camera)\n",
    "\n",
    "camera_df = camera_df.select(\n",
    "    col(\"camera_id\").cast(IntegerType()).alias(\"camera_id\"),\n",
    "    col(\"position\"),\n",
    "    col(\"speed_limit\")\n",
    ").cache() # cache to avoid recomputing the static camera_df during repeated joins with the streaming data\n",
    "\n",
    "camera_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3729ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the expected JSON schema for the Kafka message payload\n",
    "json_schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"car_plate\", StringType(), True),\n",
    "    StructField(\"camera_id\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"speed_reading\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "def create_consumer(topic):\n",
    "    \"\"\"\n",
    "    Creates and returns a structured streaming DataFrame from a Kafka topic, joined with static camera metadata.\n",
    "    \n",
    "    Parameters:\n",
    "        topic (str): The Kafka topic to subscribe to.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: A streaming DataFrame containing parsed, timestamped, and enriched event data.\n",
    "    \"\"\"\n",
    "    topic_stream = (\n",
    "        spark.readStream.format('kafka')\n",
    "        .option('kafka.bootstrap.servers', f'{hostip}:9092')\n",
    "        .option('subscribe', topic) \n",
    "        .load()\n",
    "    )\n",
    "    \n",
    "     # Parse Kafka message value (JSON), extract fields, and add camera meta data\n",
    "    modified_stream = (\n",
    "        topic_stream\n",
    "        .select(\n",
    "            from_json(col(\"value\").cast(\"string\"), json_schema).alias(\"data\")\n",
    "        )\n",
    "        .select(\"data.*\")\n",
    "        .withColumn(\"event_time\", to_timestamp(col(\"timestamp\"))) # watermark for joining work with timestamp data type\n",
    "        .withColumnRenamed(\"camera_id\", \"cam_id\") # to remove it later so only one column of camera_id\n",
    "        .join(broadcast(camera_df), col(\"cam_id\") == col(\"camera_id\"))\n",
    "        .drop(\"cam_id\")\n",
    "    )\n",
    "    \n",
    "    return modified_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "065d4bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create structured streams for each of the three camera topics\n",
    "topic_stream_cam_a_df = create_consumer(topic_1)\n",
    "topic_stream_cam_b_df = create_consumer(topic_2)\n",
    "topic_stream_cam_c_df = create_consumer(topic_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285e2ff",
   "metadata": {},
   "source": [
    "## Join between stream a and b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3df714",
   "metadata": {},
   "source": [
    "As previously mentioned, watermarking is essential for managing state during stream-stream joins. While increasing the watermark duration can help reduce data loss by allowing more late-arriving data to be matched, it also increases the state retention time, which leads to higher memory usage and makes the join operation more resource-intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec2eb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- left_event_id: string (nullable = true)\n",
      " |-- left_batch_id: integer (nullable = true)\n",
      " |-- left_car_plate: string (nullable = true)\n",
      " |-- left_timestamp: string (nullable = true)\n",
      " |-- left_speed_reading: double (nullable = true)\n",
      " |-- left_event_time: timestamp (nullable = true)\n",
      " |-- left_camera_id: integer (nullable = true)\n",
      " |-- left_position: double (nullable = true)\n",
      " |-- left_speed_limit: long (nullable = true)\n",
      " |-- right_event_id: string (nullable = true)\n",
      " |-- right_batch_id: integer (nullable = true)\n",
      " |-- right_car_plate: string (nullable = true)\n",
      " |-- right_timestamp: string (nullable = true)\n",
      " |-- right_speed_reading: double (nullable = true)\n",
      " |-- right_event_time: timestamp (nullable = true)\n",
      " |-- right_camera_id: integer (nullable = true)\n",
      " |-- right_position: double (nullable = true)\n",
      " |-- right_speed_limit: long (nullable = true)\n",
      " |-- distance_km: double (nullable = true)\n",
      " |-- time_diff_hrs: double (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- batch_id: integer (nullable = true)\n",
      " |-- car_plate: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- speed_reading: double (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- camera_id: integer (nullable = true)\n",
      " |-- position: double (nullable = true)\n",
      " |-- speed_limit: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply watermarking to each input stream to manage state and control late data retention.\n",
    "# Columns are renamed with prefixes (\"left_\" and \"right_\") to differentiate them after join\n",
    "cam_a_watermarked = (\n",
    "    topic_stream_cam_a_df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "    .select([col(c).alias(f\"left_{c}\") for c in topic_stream_cam_a_df.columns])\n",
    ") \n",
    "\n",
    "cam_b_watermarked = (\n",
    "    topic_stream_cam_b_df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "    .select([col(c).alias(f\"right_{c}\") for c in topic_stream_cam_b_df.columns])\n",
    ") \n",
    "\n",
    "\n",
    "joined_stream_a_b = cam_a_watermarked.join(\n",
    "    cam_b_watermarked,\n",
    "    expr(\"\"\"\n",
    "        left_car_plate = right_car_plate AND\n",
    "        right_event_time > left_event_time AND\n",
    "        right_event_time <= left_event_time + interval 2 minutes\n",
    "    \"\"\"),\n",
    "    \"full_outer\"\n",
    ")\n",
    "\n",
    "# From the joined stream, filter out records that have valid matches\n",
    "stream_with_avg_speed_a_b = (\n",
    "    joined_stream_a_b\n",
    "    .filter(\"left_car_plate IS NOT NULL AND right_car_plate IS NOT NULL\")\n",
    "    .withColumn(\"distance_km\", spark_abs(col(\"left_position\") - col(\"right_position\")))\n",
    "    .withColumn(\"time_diff_hrs\", (unix_timestamp(\"right_event_time\") - unix_timestamp(\"left_event_time\")) / 3600)\n",
    "    .withColumn(\"avg_speed\", col(\"distance_km\") / col(\"time_diff_hrs\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Extract unmatched records from the full outer join.\n",
    "# These represent partial records—vehicles observed at only one of the two camera points—\n",
    "# which may be joined later when more data arrives.\n",
    "unmatched_records_a_b = joined_stream_a_b.filter(\n",
    "    \"left_car_plate IS NULL OR right_car_plate IS NULL\"\n",
    ").select(\n",
    "    coalesce(col(\"left_event_id\"), col(\"right_event_id\")).alias(\"event_id\"),\n",
    "    coalesce(col(\"left_car_plate\"), col(\"right_car_plate\")).alias(\"car_plate\"),\n",
    "    coalesce(col(\"left_timestamp\"), col(\"right_timestamp\")).alias(\"timestamp\"),\n",
    "    coalesce(col(\"left_speed_reading\"), col(\"right_speed_reading\")).alias(\"speed_reading\"),\n",
    "    coalesce(col(\"left_event_time\"), col(\"right_event_time\")).alias(\"event_time\"),\n",
    "    coalesce(col(\"left_camera_id\"), col(\"right_camera_id\")).alias(\"camera_id\"),\n",
    "    coalesce(col(\"left_position\"), col(\"right_position\")).alias(\"position\"),\n",
    "    coalesce(col(\"left_speed_limit\"), col(\"right_speed_limit\")).alias(\"speed_limit\"),\n",
    ")\n",
    "    \n",
    "stream_with_avg_speed_a_b.printSchema()\n",
    "unmatched_records_a_b.printSchema()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d53b83",
   "metadata": {},
   "source": [
    "## Join stream b and c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d354296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply watermarking to each input stream to manage state and control late data retention.\n",
    "# Columns are renamed with prefixes (\"left_\" and \"right_\") to differentiate them after join\n",
    "cam_b_watermarked = (\n",
    "    topic_stream_cam_b_df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "    .select([col(c).alias(f\"left_{c}\") for c in topic_stream_cam_b_df.columns])\n",
    ") \n",
    "\n",
    "cam_c_watermarked = (\n",
    "    topic_stream_cam_c_df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "    .select([col(c).alias(f\"right_{c}\") for c in topic_stream_cam_c_df.columns])\n",
    ") \n",
    "\n",
    "joined_stream_b_c = cam_b_watermarked.join(\n",
    "    cam_c_watermarked,\n",
    "    expr(\"\"\"\n",
    "        left_car_plate = right_car_plate AND\n",
    "        right_event_time > left_event_time AND\n",
    "        right_event_time <= left_event_time + interval 2 minutes\n",
    "    \"\"\"),\n",
    "    \"full_outer\"\n",
    ")\n",
    "\n",
    "\n",
    "# From the joined stream, filter out records that have valid matches\n",
    "stream_with_avg_speed_b_c = (\n",
    "    joined_stream_b_c\n",
    "    .filter(\"left_car_plate IS NOT NULL AND right_car_plate IS NOT NULL\")\n",
    "    .withColumn(\"distance_km\", spark_abs(col(\"left_position\") - col(\"right_position\")))\n",
    "    .withColumn(\"time_diff_hrs\", (unix_timestamp(\"right_event_time\") - unix_timestamp(\"left_event_time\")) / 3600)\n",
    "    .withColumn(\"avg_speed\", col(\"distance_km\") / col(\"time_diff_hrs\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Extract unmatched records from the full outer join.\n",
    "# These represent partial records—vehicles observed at only one of the two camera points—\n",
    "# which may be joined later when more data arrives.\n",
    "unmatched_records_b_c = joined_stream_b_c.filter(\n",
    "    \"left_car_plate IS NULL OR right_car_plate IS NULL\"\n",
    ").select(\n",
    "    coalesce(col(\"left_event_id\"), col(\"right_event_id\")).alias(\"event_id\"),\n",
    "    coalesce(col(\"left_car_plate\"), col(\"right_car_plate\")).alias(\"car_plate\"),\n",
    "    coalesce(col(\"left_timestamp\"), col(\"right_timestamp\")).alias(\"timestamp\"),\n",
    "    coalesce(col(\"left_speed_reading\"), col(\"right_speed_reading\")).alias(\"speed_reading\"),\n",
    "    coalesce(col(\"left_event_time\"), col(\"right_event_time\")).alias(\"event_time\"),\n",
    "    coalesce(col(\"left_camera_id\"), col(\"right_camera_id\")).alias(\"camera_id\"),\n",
    "    coalesce(col(\"left_position\"), col(\"right_position\")).alias(\"position\"),\n",
    "    coalesce(col(\"left_speed_limit\"), col(\"right_speed_limit\")).alias(\"speed_limit\"),\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1040979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pymongo import DeleteOne,ReplaceOne\n",
    "from pymongo.errors import PyMongoError\n",
    "import time\n",
    "\n",
    "def safe_bulk_write(collection, operations, max_retries=3, delay=1):\n",
    "    \"\"\"\n",
    "    Attempt to perform a bulk write operation on a MongoDB simple retry.\n",
    "    Args:\n",
    "        collection: The MongoDB collection object to perform the bulk write on.\n",
    "        operations: A list of write operations\n",
    "        max_retries (int): Maximum number of retry attempts if the write fails. Default is 3.\n",
    "        delay (int or float): Delay in seconds between retry attempts. Default is 1 second.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    if operations == [] :\n",
    "        return\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            collection.bulk_write(operations)\n",
    "            return\n",
    "        except PyMongoError as e:\n",
    "            attempt += 1\n",
    "            print(f\"[Retry {attempt}] Failed to write to '{collection}': {e}\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "\n",
    "\n",
    "class DbWriterJoin:\n",
    "    \n",
    "    \"\"\"\n",
    "    A class used to process stream of successful joined records \n",
    "    \"\"\"\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.client = MongoClient(host= hostip, port=27017)  \n",
    "        self.db = self.client[\"fit3182_a2_db\"]\n",
    "        self.buffer = [] # to store the operation for bulk write in end of partition\n",
    "        self.drop_pair = [] # to store pair that don't violate the speed limit to be logged\n",
    "\n",
    "        return True\n",
    "\n",
    "    def process(self, row):\n",
    "        \"\"\"\n",
    "        Process a single row from the stream. If average speed exceeds the limit,\n",
    "        prepare it for insertion as a violation; otherwise, store it to print it at the end of partition.\n",
    "\n",
    "        \"\"\"\n",
    "        row_dict = row.asDict()\n",
    "     \n",
    "        if row_dict[\"avg_speed\"] > row_dict[\"right_speed_limit\"] :\n",
    "            violation_id = row_dict[\"left_event_id\"] + row_dict[\"right_event_id\"] \n",
    "    \n",
    "            record = {\n",
    "                \"violation_id\" : violation_id,\n",
    "                \"car_plate\": row_dict[\"left_car_plate\"],\n",
    "                \"camera_id_end\": row_dict[\"right_camera_id\"],\n",
    "                \"camera_id_start\": row_dict[\"left_camera_id\"],\n",
    "                \"timestamp_start\": row_dict[\"left_timestamp\"],\n",
    "                \"timestamp_end\": row_dict[\"right_timestamp\"],\n",
    "                \"speed_reading\": row_dict[\"avg_speed\"],\n",
    "                \"violation_type\" : \"average\"\n",
    "            }\n",
    "        \n",
    "            # Prepare upsert operation to avoid duplicate entries for the same violation\n",
    "            self.buffer.append(ReplaceOne({\"violation_id\" : violation_id},record,upsert = True))\n",
    "\n",
    "        else :\n",
    "            # Record that did not exceed speed limit; store for logging\n",
    "            self.drop_pair.append(row_dict)\n",
    "        \n",
    "        \n",
    "    def close(self, error):\n",
    "        if error is None  :\n",
    "            \n",
    "            # Write all buffered violations to MongoDB\n",
    "            safe_bulk_write(self.db[\"violations\"], self.buffer, max_retries=3, delay=1)\n",
    "            \n",
    "            # Extract and print left and right record contents\n",
    "            if self.drop_pair :\n",
    "                print(f'drop pairs : ')\n",
    "                for pair in self.drop_pair :\n",
    "                    left_dict = {k[5:] : v for k, v in pair.items() if k.startswith('left_')}\n",
    "                    right_dict = {k[6:] : v for k, v in pair.items() if k.startswith('right_')}\n",
    "                    print((left_dict,right_dict))\n",
    "        self.client.close()\n",
    "\n",
    "\n",
    "class DbWriterNoMatch:\n",
    "    \"\"\"\n",
    "    A class to handle writing unmatched records from a streaming join to MongoDB.\n",
    "\n",
    "    This writer processes records that did not match in a join (e.g., no corresponding left/right event)\n",
    "    and stores them in a 'no_match_records' MongoDB collection.\n",
    "    \"\"\"\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.client = MongoClient(host= hostip, port=27017)  # replace with actual host/port\n",
    "        self.db = self.client[\"fit3182_a2_db\"]\n",
    "        self.buffer = []  # to store the operation for bulk write in end of partition\n",
    "        return True\n",
    "\n",
    "    def process(self, row):\n",
    "        \"\"\"\n",
    "        Process a single unmatched record and prepare it for MongoDB upsert.\n",
    "\n",
    "        \"\"\"\n",
    "        row_dict = row.asDict()\n",
    "        \n",
    "        record = {\n",
    "            \"event_id\" : row_dict[\"event_id\"],\n",
    "            \"car_plate\": row_dict[\"car_plate\"],\n",
    "            \"timestamp\": row_dict[\"timestamp\"],\n",
    "            \"speed_reading\": row_dict[\"speed_reading\"],\n",
    "            \"event_time\" : row_dict[\"event_time\"],\n",
    "            \"camera\" : {\n",
    "                \"camera_id\": row_dict[\"camera_id\"],\n",
    "                \"position\" : row_dict[\"position\"],\n",
    "                \"speed_limit\" : row_dict[\"speed_limit\"],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Prepare upsert operation to avoid duplicate entries for the same violation\n",
    "        self.buffer.append(ReplaceOne({\"event_id\" : row_dict[\"event_id\"]}, record ,upsert = True))\n",
    "     \n",
    "    def close(self, error):\n",
    "        if error is None :\n",
    "    \n",
    "            safe_bulk_write(self.db[\"no_match_records\"], self.buffer, max_retries=3, delay=1)\n",
    "    \n",
    "        self.client.close()\n",
    "        \n",
    "class DbWriterSingle:\n",
    "    \"\"\"\n",
    "    A class for processing stream of single camera events.\n",
    "    Aside from checking instantaneous speed, it also read unmatched records from the 'no_match_records' MongoDB collection,\n",
    "    attempts to pair them base on car_plate and calculates average speed to determine possible violations.\n",
    "\n",
    "    \"\"\"\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.client = MongoClient(host= hostip, port=27017)  # replace with actual host/port\n",
    "        self.db = self.client[\"fit3182_a2_db\"]\n",
    "        self.buffer_violation = [] # Buffer for violation upserts\n",
    "        self.buffer_nomatch = [] # Buffer for deleting matched no-match records\n",
    "        self.record_to_drop = [] # Buffer for dropped record pairs (not violations)\n",
    "        return True\n",
    "\n",
    "    def process(self, row):\n",
    "        \"\"\"\n",
    "        Process a single event record to determine if it violate speed limit and also \n",
    "        try to find match in the no_match_record\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        row_dict = row.asDict()\n",
    "        \n",
    "        match_records = self.db.no_match_records.find({\"car_plate\": row_dict[\"car_plate\"]})\n",
    " \n",
    "        for record in match_records :\n",
    "        \n",
    "             # Only consider cameras that are adjacent\n",
    "            if abs( record[\"camera\"][\"camera_id\"] - row_dict[\"camera_id\"]) == 1 :\n",
    "                t1 = record[\"event_time\"]\n",
    "                t2 = row_dict[\"event_time\"]\n",
    "                dif = t1-t2\n",
    "                seconds = abs(dif.total_seconds())\n",
    "                \n",
    "                # determine which is the start point or end point\n",
    "                if record[\"camera\"][\"camera_id\"]  > row_dict[\"camera_id\"] :\n",
    "                    speed_limit = record[\"camera\"][\"speed_limit\"]\n",
    "                    camera_id_start = row_dict[\"camera_id\"]\n",
    "                    camera_id_end = record[\"camera\"][\"camera_id\"]\n",
    "                    timestamp_start = row_dict[\"timestamp\"]\n",
    "                    timestamp_end = record[\"timestamp\"]\n",
    "                else :\n",
    "                    speed_limit = row_dict[\"speed_limit\"]\n",
    "                    camera_id_start = record[\"camera\"][\"camera_id\"]\n",
    "                    camera_id_end = row_dict[\"camera_id\"]\n",
    "                    timestamp_start = record[\"timestamp\"]\n",
    "                    timestamp_end = row_dict[\"timestamp\"]\n",
    "\n",
    "                position1 = record[\"camera\"][\"position\"]\n",
    "                position2 = row_dict[\"position\"]\n",
    "                distance = abs(position1-position2)\n",
    "                avg_speed = distance / (seconds/3600)\n",
    "\n",
    "                if avg_speed > speed_limit :\n",
    "                    # Create a unique violation ID and construct a violation record\n",
    "                    violation_id = row_dict[\"event_id\"] + record[\"event_id\"] \n",
    "                                                       \n",
    "                    violation_record = {\n",
    "                        \"violation_id\" : violation_id,\n",
    "                        \"car_plate\": row_dict[\"car_plate\"],\n",
    "                        \"camera_id_start\": camera_id_start,\n",
    "                        \"camera_id_end\": camera_id_end,\n",
    "                        \"timestamp_start\": timestamp_start,\n",
    "                        \"timestamp_end\": timestamp_end,\n",
    "                        \"speed_reading\": avg_speed,\n",
    "                        \"violation_type\" : \"average\"\n",
    "                    }\n",
    "\n",
    "                    # Buffer for upsert to violations                      \n",
    "                    self.buffer_violation.append(ReplaceOne({\"violation_id\" : violation_id},violation_record ,upsert = True))\n",
    "                                                       \n",
    "                else :\n",
    "                    \n",
    "                    # collect to log it if it don't violate speed limit\n",
    "                    self.record_to_drop.append((record,row_dict))\n",
    "                                                    \n",
    "                # remove the record from the collection since it had found a proper match\n",
    "                self.buffer_nomatch.append(DeleteOne({\"_id\": record[\"_id\"]}))\n",
    "                  \n",
    "        instantaneous speed\n",
    "        if row_dict[\"speed_limit\"] < row_dict[\"speed_reading\"] :\n",
    "            \n",
    "            \n",
    "            record = {\n",
    "                \"violation_id\" : row_dict[\"event_id\"],\n",
    "                \"car_plate\": row_dict[\"car_plate\"],\n",
    "                \"camera_id_end\": row_dict[\"camera_id\"],\n",
    "                \"camera_id_start\": row_dict[\"camera_id\"],\n",
    "                \"timestamp_start\": row_dict[\"timestamp\"],\n",
    "                \"timestamp_end\": row_dict[\"timestamp\"],\n",
    "                \"speed_reading\": row_dict[\"speed_reading\"],\n",
    "                \"violation_type\" : \"instantaneous\"\n",
    "            }\n",
    "            \n",
    "             Buffer for upsert to violations\n",
    "            self.buffer_violation.append(ReplaceOne({\"violation_id\" : row_dict[\"event_id\"]},record,upsert = True))\n",
    "\n",
    "                                      \n",
    "    def close(self, error):\n",
    "        if error is None :                                               \n",
    "            safe_bulk_write(self.db[\"violations\"], self.buffer_violation, max_retries=3, delay=1)\n",
    "            safe_bulk_write(self.db[\"no_match_records\"], self.buffer_nomatch, max_retries=3, delay=1)\n",
    "            \n",
    "            # Print dropped pairs (valid matches that didn't result in a violation)\n",
    "            if self.record_to_drop :                            \n",
    "                print(f'dropped pairs : ')\n",
    "                for i,j in self.record_to_drop :\n",
    "                    # Clean up and reformat before printing\n",
    "                    i[\"camera_id\"] = i[\"camera\"][\"camera_id\"]\n",
    "                    i.pop(\"camera\")\n",
    "                    i.pop(\"_id\")\n",
    "                    i.pop(\"event_time\")\n",
    "                    j.pop(\"position\")\n",
    "                    j.pop(\"speed_limit\")\n",
    "                    j.pop(\"event_time\")\n",
    "                    print((i,j))\n",
    "                                                       \n",
    "        self.client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dbbd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine average speed violations from two segments: A - > B and B -> C\n",
    "stream_with_average = stream_with_avg_speed_a_b.union(stream_with_avg_speed_b_c)\n",
    "\n",
    "# Combine all incoming camera event streams (Camera A, B, and C) into a single stream\n",
    "union_cam_a_b_c = topic_stream_cam_a_df.union(topic_stream_cam_b_df).union(topic_stream_cam_c_df)\n",
    "\n",
    "# Merge unmatched records from both segments A -> B and B -> C\n",
    "no_match_record_union = unmatched_records_b_c.union(unmatched_records_a_b)\n",
    "\n",
    "# Define a streaming write operation for unmatched records\n",
    "no_match_record = (\n",
    "    no_match_record_union\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(DbWriterNoMatch())\n",
    ")\n",
    "\n",
    "\n",
    "# Define a streaming write for succesful join\n",
    "join_writer = ( \n",
    "    stream_with_average\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(DbWriterJoin())\n",
    ")\n",
    "\n",
    "\n",
    "# Define a streaming write for raw records directly from all three cameras\n",
    "single_record_writer = (\n",
    "    union_cam_a_b_c\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(DbWriterSingle())\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    query_1 = no_match_record.start()\n",
    "    query_2 = join_writer.start()\n",
    "    query_3 = single_record_writer.start()\n",
    "\n",
    "    \n",
    "    query_1.awaitTermination()\n",
    "    query_2.awaitTermination()\n",
    "    query_3.awaitTermination()\n",
    "\n",
    "\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopped query')\n",
    "except StreamingQueryException as exc:\n",
    "    print(exc)\n",
    "finally:\n",
    "    query_1.stop()\n",
    "    query_2.stop()\n",
    "    query_3.stop()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b85d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
